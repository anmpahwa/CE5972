{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 37: Swarm Intelligence\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Swarm Intelligence algorithms take inspiration from the collective behaviour of decentralised, self-organising systems found in nature, such a colony of ants, flock of birds, school of fish, etc. These algorithms model local interactions between agents that render the swarm to achieve its global objectives.  In particular, a swarm intelligence-based algorithm begins with a swarm - s, wherein each agent – s∈s represents a potential solution to the optimisation problem at hand. The algorithm operates iteratively, generating new agent – s^', for each agent – s, by updating agent’s attributes using update function – UF, swarm attributes – θ, and search parameters – χ. Consequently, the algorithm may replace the existing agent with the new agent, contingent on their fitness evaluations – f. Following each individual agent updates, the algorithm adjusts swarm parameters using an adaptive function – AF, to reflect the evolving characteristics of its agents. Having updated all the agents, the algorithm then identifies the best agent in the swarm and consequently updates the global best, if necessary. Finally, before moving onto the next iteration, the algorithm updates search parameters using the learning function – LF, to guide the swarm more effectively towards its objective. This process of local agent and global swarm updates continues until convergence, i.e., until the algorithm achieves a threshold level of solution quality or has spent a threshold amount of computational effort.\n",
    "\n",
    "Note, in any swam intelligence-based algorithm, the update, adaptive, and learning functions play a critical role in solution search. Specifically, the update function is responsible for generating new candidate solutions by modifying attributes of individual agents based on collective swarm attributes and algorithm’s search parameters. On the other hand, the adaptive function controls the evolution of swarm attributes based on individual agent attributes. For instance, in ant colony optimisation, the update function models the probabilistic choice of paths based on pheromone levels. Consequently, the adaptive function ensures sufficient pheromone evaporation to enable ants to explore other less-visited paths. On the other hand, in particle swarm optimisation, the update function adjusts the position and velocity of particles based on its individual experience (cognitive component) pertaining to the best solution developed by the particle (local optimal) and swarm’s collective experience (social component) describing the best solution developed the swarm (global optimal). Consequently, the adaptive function updates the inertia of the swarm, guiding it towards promising regions in the solution landscape. Notably, the learning function updates the search parameters to balance exploration and exploitation, and thus enable the algorithm to develop high-quality solutions. For instance, in ant colony optimisation, the learning function adjusts pheromone deposition and evaporation rates, whereas in particle swarm optimisation, it adjusts cognitive and social biases. Likewise, other swarm intelligence-based algorithms, such as the artificial bee colony algorithm, firefly algorithm, bat algorithm, etc., have their unique (albeit not necessarily explicit) update, adaptive, and learning operations that enable solution landscape exploration and exploitation.\n",
    "\n",
    "With the ability to emulate the collective intelligence of a population of agents, swarm intelligence-based algorithms have found wide use in optimising vehicle routing problem, traffic signals, parking systems, vehicle navigation systems, transit timetabling/scheduling, dial-a-ride problem, and traffic assignment problem, generally listed in the order of utilisation of swarm intelligence-based algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## Pseudo Code\n",
    "\n",
    "1. **Procedure** $\\text{EC}(s_o, ((\\text{PM}, \\mu), (\\text{CM}, \\lambda, \\rho), (\\text{MM}, \\epsilon), (\\text{NM}, \\sigma)))$\n",
    "2. $\\boldsymbol{s} \\leftarrow \\boldsymbol{s_o}$ &emsp; <small>// initialize the current population $\\boldsymbol{s}$ to the initial population $\\boldsymbol{s_o}$</small>\n",
    "3. $s \\leftarrow \\arg\\min \\{f(s); s \\in \\boldsymbol{s}\\}$ &emsp; <small>// find the local best solution $s$ in the current population</small>\n",
    "4. $s^* \\leftarrow s$ &emsp; <small>// set the global best solution $s^*$ to the local best solution</small>\n",
    "5. **while** $!\\text{converged}$ **do** &emsp; <small>// repeat until converged</small>\n",
    "6.  &emsp;$\\boldsymbol{s_p} \\leftarrow \\text{PM}(\\boldsymbol{s}, \\mu)$ &emsp; <small>// select $\\mu$ parent solutions from the current population using parent selection method $\\text{PM}$</small>\n",
    "7. &emsp;$\\boldsymbol{s_c} \\leftarrow \\text{CM}(\\boldsymbol{s_p}, \\lambda, \\rho)$ &emsp; <small>// with $\\rho$ parents procreating one offspring, generate $\\lambda$ offspring solutions using crossover method $\\text{CM}$</small>\n",
    "8.  &emsp;$\\boldsymbol{s_c} \\leftarrow \\text{MM}(\\boldsymbol{s_c}, \\epsilon)$ &emsp; <small>// mutate offspring solutions using mutation method $\\text{MM}$ with a small probability $\\epsilon$</small>\n",
    "9.  &emsp;$\\boldsymbol{s} \\leftarrow \\text{NM}(\\boldsymbol{s} \\cup \\boldsymbol{s_c}, \\sigma)$ &emsp; <small>// perform natural selection on the current population and their offspring using natural selection method $\\text{NM}$, accounting for diversity criteria $\\sigma$</small>\n",
    "10. &emsp;$s \\leftarrow \\arg\\min {,f(s)\\colon s \\in \\boldsymbol{s}}$ &emsp; <small>// find the local best solution from the current population</small>\n",
    "11. &emsp;**if** $f(s) < f(s^*)$ **then** &emsp; <small>// if the local best solution is better than the global best solution</small>\n",
    "12. &emsp; &emsp; $s^* \\leftarrow s$ &emsp; <small>// update the global best solution to the local best solution</small>\n",
    "13. &emsp; end if\n",
    "14. end while\n",
    "15. return $s^*$ &emsp; <small>// return the best solution</small>\n",
    "\n",
    "---\n",
    "\n",
    "## Research\n",
    "\n",
    "1. Particle Swarm Optimization (PSO)\n",
    "Inspired by the flocking behavior of birds or schooling of fish, PSO maintains a population of particles that each represent a candidate solution. These particles “fly” through the problem’s search space, guided by their own best-known position and by the best-known positions discovered by any member of the swarm. Over time, the swarm converges on promising regions, honing in on optimal or near-optimal solutions.\n",
    "\n",
    "2. Ant Colony Optimization (ACO)\n",
    "Modeled after how real ants find the shortest path to a food source, ACO uses simulated ants that travel through a problem graph, laying down and following a virtual “pheromone” trail. Paths with higher pheromone concentrations become more attractive, which in turn receives more pheromone deposits as more ants use them. This positive feedback loop biases ants toward exploiting the most successful routes found so far, while still allowing for exploration.\n",
    "\n",
    "3. Artificial Bee Colony (ABC)\n",
    "Emulating the foraging behavior of honeybees, ABC divides a bee population into employed bees, onlooker bees, and scout bees. Employed bees are assigned specific food sources (candidate solutions), which they exploit and then share information about to onlooker bees. Onlooker bees probabilistically choose which sources to exploit based on that shared information, while scout bees abandon poor solutions and search for new ones. Through this division of labor, ABC simultaneously explores and exploits good regions in the search space.\n",
    "\n",
    "4. Firefly Algorithm\n",
    "Motivated by the flashing behavior and attraction mechanisms of fireflies, this algorithm positions fireflies (candidate solutions) within a search space. The brightness of each firefly corresponds to its objective function value (fitness), while the attraction between any two fireflies is proportional to their brightness and inversely related to their distance. Brighter fireflies pull in the less bright ones, forming a swarm that converges to regions of higher fitness.\n",
    "\n",
    "5. Bat Algorithm\n",
    "The Bat Algorithm draws on the echolocation capabilities of bats to navigate and detect prey. Each bat (candidate solution) moves according to a velocity and a frequency/pulse emission rate that adjusts based on its performance. Loudness and pulse rates are dynamically tuned as bats hone in on better solutions, balancing exploration in the early stages with increasingly finer exploitation.\n",
    "\n",
    "6. Grey Wolf Optimizer (GWO)\n",
    "Based on the social hierarchy and hunting strategy of grey wolves, GWO categorizes solutions into alpha (leader), beta, delta, and omega wolves. The alpha, beta, and delta wolves guide the search by “encircling” the current best regions of the solution space. Omega wolves follow this leadership, updating their positions accordingly. The pack’s interaction and pursuit strategy mimic natural hunting phases, leading to convergence on global optima.\n",
    "\n",
    "7. Whale Optimization Algorithm (WOA)\n",
    "Inspired by the bubble-net feeding approach of humpback whales, WOA uses two main phases: exploitation (shrinking encircling mechanism) and exploration (search for prey). Candidate solutions (whales) either spiral around their best known target or search for new positions. This combination emulates how humpback whales create bubble nets to herd and capture prey.\n",
    "\n",
    "8. Cuckoo Search\n",
    "Inspired by the brood parasitism of certain cuckoo species, where they lay their eggs in the nests of other birds, Cuckoo Search uses a Lévy flight step for global exploration while directing local exploitation through comparisons of egg fitness. Poor solutions (nests) are abandoned, and new nests (candidate solutions) are created, focusing efforts on high-quality solutions in a balanced manner."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
