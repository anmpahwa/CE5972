{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 34: Evolutionary Computation\n",
    "\n",
    "```{note}\n",
    "...\n",
    "```\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Inspired from the biological process of natural evolution, the Evolutionary Computation algorithm takes a population of solutions and then iteratively evolves the population through  parent selection, crossover, mutation, and natural selection operations to find high-quality solutions. Specifically, it begins with an initial population of solutions – $s_o$, and consequently sets the current solution – $s$, and global best solution – $s^*$, both, to the local best solution from this initial population. Hereafter, in each iteration, the algorithm 1) selects $\\mu$ parent solutions using parent selection method – $\\text{PM}$, then, 2) generates $\\lambda$ offspring solutions from the selected parent solutions using crossover method – $\\text{CM}$, that combines $\\rho$ parent solutions to generate one offspring solution, 3) mutates offspring solutions using mutation method – $\\text{MM}$, with a small probability $\\epsilon$, and finally 4) performs natural selection to generate a new population of solutions (from the current population of solutions as well as their offspring solutions) using selection method – $\\text{NM}$, accounting for diversity criteria – $\\sigma$. Hereafter, the algorithm updates the current solution and the global best solution. Specifically, the algorithm sets the current solution to the local best solution regardless of their objective function evaluations – $f$. However, as the name suggests, the global best solution is only set to the local best solution if the latter is better than the former. This iterative process of search evolution continues until the algorithm achieves a threshold level of solution quality or computational effort. Upon convergence, the algorithm returns the global best solution.\n",
    "\n",
    "Note, the evolutionary operations in this algorithm – parent selection, crossover, mutation, and natural selection – are mathematical functions that emulate the respective natural evolution processes. For instance, the parent selection method utilises strategies such as tournament selection, roulette wheel selection, or rank-based selection, to stochastically select solutions based on their objective function evaluation as parent solutions for the next generation. The crossover method then combines distinct features (decision variable values) from parent solutions to generate the offspring solution. Subsequently, the mutation method introduces random changes to the offspring solutions by altering or adding noise to specific features. Finally, the natural selection method may emulate survival of the fittest using truncation or elitist mechanisms; or promote population diversity through use of crowding and sharing mechanisms. Together, these evolutionary operations enable the algorithm to exploit the solution landscape to intensify the search as well as explore the solution landscape to diversify the search. This use of evolutionary operations on a population of solutions to search for high-quality solutions thus makes Evolutionary Computation a very robust algorithm. Nonetheless, the success of this algorithm is contingent on deliberate selection of evolutionary operations ($\\text{PM}$, $\\text{CM}$, $\\text{MM}$, $\\text{NM}$) and precise fine-tuning of evolutionary parameters ($\\mu$, $\\lambda$, $\\rho$, $\\epsilon$, $\\sigma$), tailored to the specific problem at hand.\n",
    "\n",
    "The discussion here provides a general description of the Evolutionary Computation algorithm – a broad term that encompasses Evolutionary Strategies, Genetic Algorithm, Genetic Programming, and Evolutionary Programming. In practice, the exact algorithm implementation and associated semantics are contingent on the nature of the problem and characteristics of the solution domain space. For instance, real-valued continuous optimisation problems employ Evolutionary Strategies which are particularly suited to fine-tune continuous variables. However, combinatorial optimisation problems, wherein the objective is to find an optimal combination (arrangement, sequence, or order) of elements, necessitate the use of Genetic Algorithms that encode the solution as a string of elements (chromosomes). On the other hand, structural design problems develop an optimal hierarchical configuration of elements through Genetic Programming that models solution as a tree, where nodes represent information, and branches represent relationship between them. While problems that aim emulate optimal system behaviour utilise Evolutionary Programming with solutions represented as candidate strategies, finite state machines, or stochastic models to facilitate adaptive behaviour. \n",
    "\n",
    "Considering the extensive nature of Evolutionary Computation algorithms to address diverse range of problems, it has found immense success in optimising parking systems, vehicle navigation systems, transit timetabling and scheduling, dial-a-ride problem, vehicle routing problem, traffic signals, and traffic assignment problem, listed in increasing order of problem scope.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Pseudo Code\n",
    "\n",
    "1. **Procedure** $\\text{EC}(s_o, ((\\text{SM}, \\mu), (\\text{CM}, \\lambda, \\rho), (\\text{MM}, \\epsilon), (\\text{NM}, \\sigma)))$\n",
    "2. $\\boldsymbol{s} ← \\boldsymbol{s_o}$ &emsp;<small>// initialise the current population $\\boldsymbol{s}$ to the initial population $\\boldsymbol{s_o}$</small>\n",
    "3. $s ← \\text{argmin} \\{f(s); s ∈ \\boldsymbol{s}\\}$ &emsp;<small>// find the local best solution in the current population</small>\n",
    "4. $s^* ← s$ &emsp;<small>// set the global best solution $s^*$ to the local best solution</small>\n",
    "5. **while** $!\\text{converged}$ **do** &emsp;<small>// repeat until converged</small>\n",
    "6. &emsp;$\\boldsymbol{s_p} ← \\text{SM}(\\boldsymbol{s}, \\mu)$ &emsp;<small>// select $\\mu$ parent solutions from the current population using selection method $\\text{SM}$</small>\n",
    "7. &emsp;$\\boldsymbol{s_c} ← \\text{CM}(\\boldsymbol{s_p}, \\lambda, \\rho)$ &emsp;<small>// with $\\rho$ parents procreating one offspring, generate $\\lambda$ offspring solutions using crossover method $\\text{CM}$</small>\n",
    "8. &emsp;$\\boldsymbol{s_c} ← \\text{MM}(\\boldsymbol{s_c}, \\epsilon)$ &emsp;<small>// mutate selected offspring solutions using mutation method $\\text{MM}$ with a small probability $\\epsilon$</small>\n",
    "9. &emsp;$\\boldsymbol{s} ← \\text{NM}(\\boldsymbol{s} ∪ \\boldsymbol{s_c}, \\sigma)$ &emsp;<small>// perform natural selection on the current population and their offsprings using natural selection method $\\text{NM}$, accounting for diversity criteria $\\sigma$</small>\n",
    "10. &emsp;$s ← \\text{argmin} \\{f(s); s ∈ \\boldsymbol{s}\\}$ &emsp;<small>// find the local best solution from the current population</small>\n",
    "11. &emsp;**if** $f(s) < f(s^*)$ **then** &emsp;<small>// if the local best solution is better than the global best solution</small>\n",
    "12. &emsp;&emsp;$s^* ← s$ &emsp;<small>// update the global best solution to the local best solution</small>\n",
    "13. &emsp;**end if**\n",
    "14. **end while**\n",
    "15. **return** $s^*$ &emsp;<small>// return the best solution</small>\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "---\n",
    "\n",
    "## Case Study\n",
    "\n",
    "---\n",
    "\n",
    "```{note}\n",
    "...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
